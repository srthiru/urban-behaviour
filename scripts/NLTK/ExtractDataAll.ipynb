{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"ExtractDataAll.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"5pzq2GeLSBWG","outputId":"6c6e798a-6877-45f7-ee3d-38889fe24494"},"source":["import json\n","import pandas as pd\n","import nltk\n","from collections import Counter\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()\n","#nltk.download('vader_lexicon')\n","\n","import os.path\n","import tarfile\n","\n","import string\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","porter = PorterStemmer()\n","stop_words = stopwords.words('english')\n","\n","# ‚Ä¢    October 31 ‚Äì 7 pm ‚Äì village Halloween parade ‚Äì 6th avenue at Canal Street between 6.30 ‚Äì 8.30\n","# ‚Ä¢    Diwali at times square ‚Äì October 13th\n","# ‚Ä¢    Columbus day parade ‚Äì October 8th ‚Äì fifth avenue from 44th to 72nd \n","# ‚Ä¢    October 10th ‚Äì Slice out hunger/pizza party ‚Äì St Anthony‚Äôs Church/155 Sullivan\n","\n","\n","year_month = \"2018-10-\"\n","dates = [\"8\",\"10\",\"13\",\"31\"]\n","folders = [year_month + \"0\" + date if len(date) == 1 else year_month + date for date in dates]\n","print(folders)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["['2018-10-08', '2018-10-10', '2018-10-13', '2018-10-31']\n"]}]},{"cell_type":"code","metadata":{"id":"O_jUD97sSBWP"},"source":["fileName = \"twitter_us_2018-10-01_00h.json\"\n","\n","# with open(fileName, encoding=\"utf8\") as file:\n","#     #json_file = file.read()\n","#     tweets = json.load(file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tInGKnzvSBWQ"},"source":["def bag_of_words(df_col): #tweets_df['text']\n","    bow = df_col.apply(lambda x: Counter(x.split(\" \") if type(x) == str else x))\n","    return bow\n","\n","def sentiment(df_col): #tweets_df['text']\n","    nltk_sentiment = df_col.apply(lambda x: sia.polarity_scores(x))\n","    return nltk_sentiment\n","\n","def segregate_sentiment(nltk_sentiment):\n","    pos_sentiment = nltk_sentiment.apply(lambda x: x['pos'])\n","    neg_sentiment = nltk_sentiment.apply(lambda x: x['neg'])\n","    neu_sentiment = nltk_sentiment.apply(lambda x: x['neu'])\n","    return pos_sentiment, neg_sentiment, neu_sentiment\n","\n","def classify_sentiment(x):\n","    if x['pos'] > 0.2:\n","        return \"Positive\"\n","    if x['neg'] > 0.2:\n","        return \"Negative\"\n","    if x['neu'] > 0.2:\n","        return \"Neutral\"\n","    return \"Mixed\"\n","\n","def classify(df_col):\n","    return df_col.apply(lambda x: classify_sentiment(x))\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = \"\".join([char for char in text if char not in string.punctuation])\n","    words = word_tokenize(text)\n","    filtered_words = [word for word in words if word not in stop_words]\n","    stemmed_words = [porter.stem(word) for word in filtered_words]\n","    return stemmed_words\n","\n","def preprocess(df_col):\n","    return df_col.apply(lambda x: preprocess_text(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8ctUhWWSBWQ"},"source":["def process_df(tweets_df):\n","    processed_df = pd.DataFrame()\n","    processed_df['id'] = tweets_df['id']\n","    processed_df['text'] = tweets_df['text']\n","    sentiment_col = sentiment(tweets_df['text'])\n","    processed_df['Sentiment'] = classify(sentiment_col)\n","    processed_df['Extracted Keywords'] = preprocess(tweets_df['text'])\n","    processed_df['lang'] = tweets_df['lang']\n","    processed_df['retweet_count'] = tweets_df['retweet_count']\n","    processed_df['coordinates'] = tweets_df['coordinates']\n","    return processed_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bg9YegHRSBWR","outputId":"064c89dd-a696-4655-fe4a-a1407fead9f4"},"source":["hours = [\"0\" + str(hr) if len(str(hr)) == 1 else str(hr) for hr in range(24)]\n","\n","def check_files_or_create(file_paths):\n","    for file_path in file_paths:\n","        if os.path.isfile(file_path + \".tar.gz\"):\n","            my_tar = tarfile.open(file_path + \".tar.gz\")\n","            folder = file_path.split('/')[0]\n","            my_tar.extractall(folder) # Extract here\n","            my_tar.close()\n","    \n","for folder in folders:\n","    file_paths = [folder + \"/\" + \"twitter_us_\" + folder + \"_\" + hour + \"h.json\" for hour in hours]\n","    print(file_paths)\n","    check_files_or_create(file_paths)\n","    \n","    daily_df = pd.DataFrame()\n","    for hr, file in enumerate(file_paths):\n","        json_file = file.split(\"/\")[0] + \"/data/\" + file.split(\"/\")[0] + \"/\" + file.split(\"/\")[1] \n","        tweets = []\n","        for line in open(json_file, encoding=\"utf8\"):\n","            tweets.append(json.loads(line))\n","        tweets_df = pd.DataFrame(tweets)\n","        processed_df = process_df(tweets_df)\n","        if not os.path.exists(\"output/\" + file.split(\"/\")[0]):\n","            os.makedirs(\"output/\" + file.split(\"/\")[0])\n","        processed_df.to_excel(\"output/\" + file.split(\".\")[0] + \".xlsx\")\n","        daily_df = daily_df.append(processed_df)\n","        grouping = processed_df.set_index('Sentiment')['Extracted Keywords'].apply(pd.Series).stack().groupby(level=0).value_counts()\n","        grouping = grouping[grouping > 15]\n","        grouping.to_excel(\"output/\" + folder + \"/hourly_\" + file.split(\"/\")[1].split(\".\")[0] + \".xlsx\")\n","    daily = daily_df.set_index('Sentiment')['Extracted Keywords'].apply(pd.Series).stack().groupby(level=0).value_counts()\n","    daily = daily[daily > 100]\n","    daily.to_excel('output/' + folder + '/daily.xlsx')\n","#   daily_df[hr] = bag_of_words(processed_df['Extracted Keywords'])\n","#   daily_df.to_excel(\"output/\" + folder + \"/hourly.xlsx\")"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["['2018-10-08/twitter_us_2018-10-08_00h.json', '2018-10-08/twitter_us_2018-10-08_01h.json', '2018-10-08/twitter_us_2018-10-08_02h.json', '2018-10-08/twitter_us_2018-10-08_03h.json', '2018-10-08/twitter_us_2018-10-08_04h.json', '2018-10-08/twitter_us_2018-10-08_05h.json', '2018-10-08/twitter_us_2018-10-08_06h.json', '2018-10-08/twitter_us_2018-10-08_07h.json', '2018-10-08/twitter_us_2018-10-08_08h.json', '2018-10-08/twitter_us_2018-10-08_09h.json', '2018-10-08/twitter_us_2018-10-08_10h.json', '2018-10-08/twitter_us_2018-10-08_11h.json', '2018-10-08/twitter_us_2018-10-08_12h.json', '2018-10-08/twitter_us_2018-10-08_13h.json', '2018-10-08/twitter_us_2018-10-08_14h.json', '2018-10-08/twitter_us_2018-10-08_15h.json', '2018-10-08/twitter_us_2018-10-08_16h.json', '2018-10-08/twitter_us_2018-10-08_17h.json', '2018-10-08/twitter_us_2018-10-08_18h.json', '2018-10-08/twitter_us_2018-10-08_19h.json', '2018-10-08/twitter_us_2018-10-08_20h.json', '2018-10-08/twitter_us_2018-10-08_21h.json', '2018-10-08/twitter_us_2018-10-08_22h.json', '2018-10-08/twitter_us_2018-10-08_23h.json']\n","['2018-10-10/twitter_us_2018-10-10_00h.json', '2018-10-10/twitter_us_2018-10-10_01h.json', '2018-10-10/twitter_us_2018-10-10_02h.json', '2018-10-10/twitter_us_2018-10-10_03h.json', '2018-10-10/twitter_us_2018-10-10_04h.json', '2018-10-10/twitter_us_2018-10-10_05h.json', '2018-10-10/twitter_us_2018-10-10_06h.json', '2018-10-10/twitter_us_2018-10-10_07h.json', '2018-10-10/twitter_us_2018-10-10_08h.json', '2018-10-10/twitter_us_2018-10-10_09h.json', '2018-10-10/twitter_us_2018-10-10_10h.json', '2018-10-10/twitter_us_2018-10-10_11h.json', '2018-10-10/twitter_us_2018-10-10_12h.json', '2018-10-10/twitter_us_2018-10-10_13h.json', '2018-10-10/twitter_us_2018-10-10_14h.json', '2018-10-10/twitter_us_2018-10-10_15h.json', '2018-10-10/twitter_us_2018-10-10_16h.json', '2018-10-10/twitter_us_2018-10-10_17h.json', '2018-10-10/twitter_us_2018-10-10_18h.json', '2018-10-10/twitter_us_2018-10-10_19h.json', '2018-10-10/twitter_us_2018-10-10_20h.json', '2018-10-10/twitter_us_2018-10-10_21h.json', '2018-10-10/twitter_us_2018-10-10_22h.json', '2018-10-10/twitter_us_2018-10-10_23h.json']\n","['2018-10-13/twitter_us_2018-10-13_00h.json', '2018-10-13/twitter_us_2018-10-13_01h.json', '2018-10-13/twitter_us_2018-10-13_02h.json', '2018-10-13/twitter_us_2018-10-13_03h.json', '2018-10-13/twitter_us_2018-10-13_04h.json', '2018-10-13/twitter_us_2018-10-13_05h.json', '2018-10-13/twitter_us_2018-10-13_06h.json', '2018-10-13/twitter_us_2018-10-13_07h.json', '2018-10-13/twitter_us_2018-10-13_08h.json', '2018-10-13/twitter_us_2018-10-13_09h.json', '2018-10-13/twitter_us_2018-10-13_10h.json', '2018-10-13/twitter_us_2018-10-13_11h.json', '2018-10-13/twitter_us_2018-10-13_12h.json', '2018-10-13/twitter_us_2018-10-13_13h.json', '2018-10-13/twitter_us_2018-10-13_14h.json', '2018-10-13/twitter_us_2018-10-13_15h.json', '2018-10-13/twitter_us_2018-10-13_16h.json', '2018-10-13/twitter_us_2018-10-13_17h.json', '2018-10-13/twitter_us_2018-10-13_18h.json', '2018-10-13/twitter_us_2018-10-13_19h.json', '2018-10-13/twitter_us_2018-10-13_20h.json', '2018-10-13/twitter_us_2018-10-13_21h.json', '2018-10-13/twitter_us_2018-10-13_22h.json', '2018-10-13/twitter_us_2018-10-13_23h.json']\n","['2018-10-31/twitter_us_2018-10-31_00h.json', '2018-10-31/twitter_us_2018-10-31_01h.json', '2018-10-31/twitter_us_2018-10-31_02h.json', '2018-10-31/twitter_us_2018-10-31_03h.json', '2018-10-31/twitter_us_2018-10-31_04h.json', '2018-10-31/twitter_us_2018-10-31_05h.json', '2018-10-31/twitter_us_2018-10-31_06h.json', '2018-10-31/twitter_us_2018-10-31_07h.json', '2018-10-31/twitter_us_2018-10-31_08h.json', '2018-10-31/twitter_us_2018-10-31_09h.json', '2018-10-31/twitter_us_2018-10-31_10h.json', '2018-10-31/twitter_us_2018-10-31_11h.json', '2018-10-31/twitter_us_2018-10-31_12h.json', '2018-10-31/twitter_us_2018-10-31_13h.json', '2018-10-31/twitter_us_2018-10-31_14h.json', '2018-10-31/twitter_us_2018-10-31_15h.json', '2018-10-31/twitter_us_2018-10-31_16h.json', '2018-10-31/twitter_us_2018-10-31_17h.json', '2018-10-31/twitter_us_2018-10-31_18h.json', '2018-10-31/twitter_us_2018-10-31_19h.json', '2018-10-31/twitter_us_2018-10-31_20h.json', '2018-10-31/twitter_us_2018-10-31_21h.json', '2018-10-31/twitter_us_2018-10-31_22h.json', '2018-10-31/twitter_us_2018-10-31_23h.json']\n"]}]},{"cell_type":"code","metadata":{"id":"T8jbFq1GSBWS","outputId":"dcff47ac-8b70-4156-af07-6c52560bd2f2"},"source":["grouping"],"execution_count":null,"outputs":[{"data":{"text/plain":["Sentiment           \n","Negative   traffic      10\n","           new           8\n","           halloween     7\n","           york          7\n","           delay         6\n","                        ..\n","Positive   ü§†             1\n","           ü§¥üèºüë∏üèº          1\n","           ü•ò             1\n","           ü•û             1\n","           ü¶á             1\n","Length: 4817, dtype: int64"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"bQ6NJjwKSBWS"},"source":[""],"execution_count":null,"outputs":[]}]}